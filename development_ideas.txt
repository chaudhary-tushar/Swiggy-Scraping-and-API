    make csv of links and details in same lines and remove pre_name files
    pooling with 4 threads with in the way
    sql db created with two tables right now 
    design a more industrialized folder structure and document everything.

    write database automation such that unique value are inserted everytime.

    stop scraping when swiggy glitches or stops maximum requests reached.

    rewrite csv scraping automation.

    there are 65000 restaurant_links but only 26187 menus.

    rewrite checkdups library.

    1807-247

    working on testing mech (diff and rnafunc ) issue posting restaurant menu in different city folder


     # Find all the restaurant names on the page and print them to the console
            restaurant_names = soup.find_all('div', {'class': '_3XX_A'})
            restn=[]
            fp=Folder()
            file_path0=fp.getdbfile("details",H_name)
            if os.path.isfile(file_path0):
                with open(file_path0, 'r',encoding='utf-8') as file:
                    # Write a string to the file
                    for name in file:
                        restn.append(name)
                    file.close()
                        
                with open(file_path0, 'a',encoding='utf-8') as file:
                    # Write a string to the file
                    for name in restaurant_names:
                        check=name.text+"\n"
                        if check not in restn:
                            line=name.text
                            name=line.replace("Promoted","")
                            restn.append(name)
                            file.write(name+'\n')
            else:            
                with open(file_path0, 'w',encoding='utf-8') as file:
                    # Write a string to the file
                    for name in restaurant_names:
                        line=name.text
                        name=line.replace("Promoted","")
                        restn.append(name)
                        file.write(name+'\n')


            my_div = soup.find('div', {'class': 'nDVxx'})

            # Find all the links within the div
            links = my_div.find_all('a')

            # Loop through each link and print its URL
            restl=[]
            prename=[]
            file_path1=fp.getdbfile("links",H_name)
            
            file_path2=fp.getdbfile("pre",H_name)
            if os.path.isfile(file_path1) and os.path.isfile(file_path2):
                with open(file_path1, 'r',encoding='utf-16') as file1,open(file_path2,'r',encoding='utf-16') as file2:
                    for line in file1:
                        restl.append(line)
                    for line1 in file2:
                        prename.append(line1)
                    file1.close()
                    file2.close()
                with open(file_path1, 'a',encoding='utf-16') as file1,open(file_path2,'a',encoding='utf-16') as file2:
                    for link in links:
                        if(link.get('href')!=None and link.get('href')[0:5]=="/rest"  ): 
                            lkd="https://www.swiggy.com"+link.get('href')
                            last_slash_index = lkd.rfind('/')
                            nline=lkd[last_slash_index + 1:last_slash_index +20]
                            neline=nline.replace("-","_")
                            checkpre=neline+"\n"
                            if checkpre not in prename:
                                file2.write(neline+'\n')
                                prename.append(neline)
                            checklink=lkd+"\n"
                            if checklink not in restl:
                                restl.append(lkd)
                                file1.write(lkd+'\n') 
            
            else:
                with open(file_path1, 'w',encoding='utf-16') as file1,open(file_path2,'w',encoding='utf-16') as file2:
                    for link in links:
                        if(link.get('href')!=None and link.get('href')[0:5]=="/rest"  ): 
                            lkd="https://www.swiggy.com"+link.get('href')
                            last_slash_index = lkd.rfind('/')
                            nline=lkd[last_slash_index + 1:last_slash_index +20]
                            neline=nline.replace("-","_")
                            file2.write(neline+'\n')
                            prename.append(neline)
                            restl.append(lkd)
                            file1.write(lkd+'\n')
                            #print(lkd)
            mv=0
            qre=re.search(r'\d+',countres)
            countres=int(qre.group())
            file_path3=fp.getdbfile("tot",H_name)  
            if os.path.isfile(file_path3):  
                with open(file_path3,'r',encoding='utf-8') as filem:
                    for line in filem:
                        if "listed" in line:
                            match = re.search(r'\d+',line)
                            matchq=int(match.group())
                            mv=matchq       
                    filem.close()
                
                if mv<countres:
                    with open (file_path3,'w',encoding='utf-8') as fileq:
                        fileq.write(f"Total restaurants listed in {H_name} = {countres}\n")
                        fileq.write(f"Total restaurant links in {H_name}= {len(restl)}\n")
                        fileq.close()
                        
                else:
                    with open (file_path3,'a',encoding='utf-8') as fileq:
                        fileq.write(f"Total restaurant links in {H_name}= {len(restl)}\n")
                        fileq.close()
            else:
                with open (file_path3,'w',encoding='utf-8') as fileq:
                        fileq.write(f"Total restaurants listed in {H_name} = {countres}\n")
                        fileq.write(f"Total restaurant links in {H_name}= {len(restl)}\n")
                        fileq.close()